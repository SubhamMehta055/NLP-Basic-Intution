{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f6fcc57-bad4-432e-a832-1faf15f5b0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#nltk'\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk  #nltk library for converting into Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9979fca7-6b1f-4e45-9c51-c96705e047ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#giving corpus (paragragh)\n",
    "corpus= '''\n",
    "My name is Subham.\n",
    "Iam from Chandigarh!\n",
    "May you are doing good today!\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "767bc738-903a-4999-bde1-88b1d546bf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "My name is Subham.\n",
      "Iam from Chandigarh!\n",
      "May you are doing good today!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adf84ee-ec60-4186-be5a-8f1660ceccfd",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization: Splitting text into smaller units like words or sentences.\n",
    "    \n",
    "1)Converting paragraph --> sentence using import sent_tokenize (for sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac5fdf18-6e29-4458-8346-03c3e5ab8652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\subham.mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "#This will ensure that the punkt resource is available for NLTK to use for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3566ac20-9755-4aaf-b195-73f84f8343fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize   #sentence tokenize\n",
    "documents= sent_tokenize(corpus) #this will convert corpus into multiple sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dac9a10b-f772-4707-801f-2f54962c103d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nMy name is Subham.',\n",
       " 'Iam from Chandigarh!',\n",
       " 'May you are doing good today!']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddb7e5e5-2d83-47c3-a0f2-e01a500ee21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "My name is Subham.\n",
      "Iam from Chandigarh!\n",
      "May you are doing good today!\n"
     ]
    }
   ],
   "source": [
    "#printing the document using python\n",
    "for sent in documents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "786d7bec-7cb0-4b94-a642-694c0d786167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cc45d7-c655-49c0-99e6-467dfe2d3e96",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "2)Converting paragraph --> words\n",
    "\n",
    "3)Converting Sentence --> words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "162f4bf3-9461-46f6-822c-c2c33b772002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Subham',\n",
       " '.',\n",
       " 'Iam',\n",
       " 'from',\n",
       " 'Chandigarh',\n",
       " '!',\n",
       " 'May',\n",
       " 'you',\n",
       " 'are',\n",
       " 'doing',\n",
       " 'good',\n",
       " 'today',\n",
       " '!']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize  \n",
    "word_tokenize(corpus) #paragraph to words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44272a9e-850d-41d8-a74f-20502083c47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'name', 'is', 'Subham', '.']\n",
      "['Iam', 'from', 'Chandigarh', '!']\n",
      "['May', 'you', 'are', 'doing', 'good', 'today', '!']\n"
     ]
    }
   ],
   "source": [
    "#document to words using python\n",
    "for sent in documents:\n",
    "    print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50fd0390-a75f-4094-b52a-92114b2ff437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Subham',\n",
       " '.',\n",
       " 'Iam',\n",
       " 'from',\n",
       " 'Chandigarh',\n",
       " '!',\n",
       " 'May',\n",
       " 'you',\n",
       " 'are',\n",
       " 'doing',\n",
       " 'good',\n",
       " 'today',\n",
       " '!']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for consider punctuations as well \n",
    "from nltk.tokenize import wordpunct_tokenize \n",
    "wordpunct_tokenize(corpus)  #like your's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f8bfe3c-73e3-4798-8e1d-e5f4949acd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer= TreebankWordTokenizer()\n",
    "#in this full stop(.) will not be separate word except the last one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "274070d0-bfbb-4266-bc37-ff2fee380411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My',\n",
       " 'name',\n",
       " 'is',\n",
       " 'Subham.',\n",
       " 'Iam',\n",
       " 'from',\n",
       " 'Chandigarh',\n",
       " '!',\n",
       " 'May',\n",
       " 'you',\n",
       " 'are',\n",
       " 'doing',\n",
       " 'good',\n",
       " 'today',\n",
       " '!']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b89e20-678e-4154-889d-9e2ad7abd389",
   "metadata": {},
   "source": [
    "So generally we use sent_tokenize and word_tokenize in most of the cases .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcb18f7-fee2-43dc-96f0-636baa3f35ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4973c06f-9bf0-4ded-a508-c47f1277ef01",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Stemming: Reducing words to their base form (e.g., \"running\" â†’ \"run\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5e17c9d-400b-4c07-a7b7-65be33593639",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list= ['eating','eats','eaten','writing','writes','profitable','programm','gone','beautiful','greedy']\n",
    "\n",
    "#Now we have types of Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4508a608-0b65-48e3-ab7e-2bc7dbc8bac9",
   "metadata": {},
   "source": [
    "## PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60e9c6f4-632a-41e8-97d1-76f73f738fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e32de064-5d84-47c8-9ea2-13cebe66c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stem=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24d980ba-1d43-46bb-af77-5dc73acae63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating ---> eat\n",
      "eats ---> eat\n",
      "eaten ---> eaten\n",
      "writing ---> write\n",
      "writes ---> write\n",
      "profitable ---> profit\n",
      "programm ---> programm\n",
      "gone ---> gone\n",
      "beautiful ---> beauti\n",
      "greedy ---> greedi\n"
     ]
    }
   ],
   "source": [
    "for word in words_list:\n",
    "    print(f\"{word} ---> {porter_stem.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe767007-6d39-4f2a-a888-47a8b22279c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## if we see above some of the text is not giving proper stem word using stemming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b08a10-768f-4f50-bdf2-5e65b2599a06",
   "metadata": {},
   "source": [
    "## RegexpStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "734b562c-8017-49c1-b3d1-6f8a69c1fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import RegexpStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "408d1d79-d76c-42bf-92ad-a61657c077f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_stem= RegexpStemmer('ing$|s$|e$|able$', min=4)  #so it will remove these suffiexs from substring and $ make sure the regexp i.e. ing /s/e/able are suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2cf18472-dc09-409b-b5f7-7feb1155f47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating --> eat\n",
      "eats --> eat\n",
      "eaten --> eaten\n",
      "writing --> writ\n",
      "writes --> write\n",
      "profitable --> profit\n",
      "programm --> programm\n",
      "gone --> gon\n",
      "beautiful --> beautiful\n",
      "greedy --> greedy\n"
     ]
    }
   ],
   "source": [
    "for word in words_list:\n",
    "    print(f\"{word} --> {reg_stem.stem(word)}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c96de-4234-4cc9-b8c7-a846a8e37ff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e2dce21-a768-45ba-8d58-5799bf7ad171",
   "metadata": {},
   "source": [
    "## Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eec7ccf2-237a-48f7-beb4-ad745375ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "daeb12d9-106c-4167-91d2-7733104db3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball_stem=SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "625c1b27-29a0-4259-8842-f7a1776968b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating --> eat\n",
      "eats --> eat\n",
      "eaten --> eaten\n",
      "writing --> write\n",
      "writes --> write\n",
      "profitable --> profit\n",
      "programm --> programm\n",
      "gone --> gone\n",
      "beautiful --> beauti\n",
      "greedy --> greedi\n"
     ]
    }
   ],
   "source": [
    "for word in words_list:\n",
    "    print(f\"{word} --> {snowball_stem.stem(word)}\")    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b4b237-b14b-42cf-bdff-c824ab39a628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ade37ff-d5bb-48b9-a1d5-7aa8eabbbb70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fairli', 'sportingli')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##lets see difference between Porter Stem and Snowball Stemmer\n",
    "\n",
    "porter_stem.stem(\"fairly\") ,porter_stem.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5284058-1c16-4d7d-b2fb-b723da8c08a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fair', 'sport')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_stem.stem(\"fairly\") ,snowball_stem.stem(\"sportingly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6ac725f-8cda-42a6-9211-4a0a29e9fcaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'goe'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snowball_stem.stem('goes') #it is not giving proper word stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a09e6dfc-0c36-4913-b759-81ce66183613",
   "metadata": {},
   "outputs": [],
   "source": [
    "## it shows the snowball stemmer is more accurate than the porter stemmer but still both are not that accurate"
   ]
  },
  {
   "cell_type": "raw",
   "id": "52568d16-0f53-4a65-bed4-08d1987eed50",
   "metadata": {},
   "source": [
    "So for cleaning the text we do Stemming which is part of Text-preprocessing in NLP .But for use cases like Chatbot we can't use this Stemming for that we have \n",
    "Lemmatization which solves the issue which we are facing in Stemming i.e. not getting proper word stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209eee20-59b9-4c3b-a942-a5d303cf7f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9228330-d31c-4068-8c07-1f3b3fc14790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d9af4a6-9453-4635-8df6-e60bf02b7059",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Use cases:\n",
    "1) Q&A\n",
    "2) Chatbots\n",
    "3) text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c71057fc-db27-4ddf-81e7-11ee6b532fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\subham.mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "98ed2871-2c4e-4aa6-bff3-34a3fc059be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmitizer = WordNetLemmatizer() #creating the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a8c9a06f-356f-4205-a958-b96fde2bbfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmitizer.lemmatize(\"going\" ,pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ded4b88-c8db-4b76-a8cd-df8d3027a300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos = Part Of Speech tag. \\n    Valid options are `\"n\"` for nouns,\\n    `\"v\"` for verbs, `\"a\"` for adjectives, `\"r\"` for adverbs and `\"s\"`\\n    for satellite adjectives.\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''pos = Part Of Speech tag. \n",
    "    Valid options are `\"n\"` for nouns,\n",
    "    `\"v\"` for verbs, `\"a\"` for adjectives, `\"r\"` for adverbs and `\"s\"`\n",
    "    for satellite adjectives.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f62cf78-83e3-4932-92d2-e3a9c247505d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_list=['eating','eats','eaten','writing','writes','profitable','programm','gone','beautiful','greedy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b550798e-6dc8-4e2a-8348-209c0cfc29f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eating --> eat\n",
      "eats --> eat\n",
      "eaten --> eat\n",
      "writing --> write\n",
      "writes --> write\n",
      "profitable --> profitable\n",
      "programm --> programm\n",
      "gone --> go\n",
      "beautiful --> beautiful\n",
      "greedy --> greedy\n"
     ]
    }
   ],
   "source": [
    "for word in words_list:\n",
    "    print(f\"{word} --> {lemmitizer.lemmatize(word,pos='v')}\")  #taking pos as verb not noun(by default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f615f32c-e08d-4a68-b69b-974dafeebee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fairly'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmitizer.lemmatize(\"fairly\" ,pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fb16b9a4-9c62-4f8c-b990-c6d750b26999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sportingly'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmitizer.lemmatize(\"sportingly\" ,pos='v') #performing good atleast giving same word without changing the real word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7ce7ef-16f7-4278-b3b2-b1ddbc614f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a09401-b281-4b6a-93a4-cfafad718282",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc2077e2-35b4-45cf-adba-ffeaa5dbdce6",
   "metadata": {},
   "source": [
    "## StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d0c040-9ac4-493c-bc75-28aafa6db6e4",
   "metadata": {},
   "source": [
    "Eliminating common words that donâ€™t contribute much to the meaning (e.g., \"is\", \"the\",\"their\",\"why\",\"from\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9b3ef49b-e692-4a0c-8c60-e00b8bc92c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "49f62bdf-522b-428e-8cf1-681c8b85dcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\subham.mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b5b0efde-91a6-46eb-881c-e476bddaa383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english') # this is for english language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0fc8340b-135d-4ed6-8987-b3df14ef9961",
   "metadata": {},
   "outputs": [],
   "source": [
    "## It is always a good way to create your own stopwords and try to remove all those words from the paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "00367247-8db1-4f31-bc12-bad8831a275e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['au',\n",
       " 'aux',\n",
       " 'avec',\n",
       " 'ce',\n",
       " 'ces',\n",
       " 'dans',\n",
       " 'de',\n",
       " 'des',\n",
       " 'du',\n",
       " 'elle',\n",
       " 'en',\n",
       " 'et',\n",
       " 'eux',\n",
       " 'il',\n",
       " 'ils',\n",
       " 'je',\n",
       " 'la',\n",
       " 'le',\n",
       " 'les',\n",
       " 'leur',\n",
       " 'lui',\n",
       " 'ma',\n",
       " 'mais',\n",
       " 'me',\n",
       " 'mÃªme',\n",
       " 'mes',\n",
       " 'moi',\n",
       " 'mon',\n",
       " 'ne',\n",
       " 'nos',\n",
       " 'notre',\n",
       " 'nous',\n",
       " 'on',\n",
       " 'ou',\n",
       " 'par',\n",
       " 'pas',\n",
       " 'pour',\n",
       " 'qu',\n",
       " 'que',\n",
       " 'qui',\n",
       " 'sa',\n",
       " 'se',\n",
       " 'ses',\n",
       " 'son',\n",
       " 'sur',\n",
       " 'ta',\n",
       " 'te',\n",
       " 'tes',\n",
       " 'toi',\n",
       " 'ton',\n",
       " 'tu',\n",
       " 'un',\n",
       " 'une',\n",
       " 'vos',\n",
       " 'votre',\n",
       " 'vous',\n",
       " 'c',\n",
       " 'd',\n",
       " 'j',\n",
       " 'l',\n",
       " 'Ã ',\n",
       " 'm',\n",
       " 'n',\n",
       " 's',\n",
       " 't',\n",
       " 'y',\n",
       " 'Ã©tÃ©',\n",
       " 'Ã©tÃ©e',\n",
       " 'Ã©tÃ©es',\n",
       " 'Ã©tÃ©s',\n",
       " 'Ã©tant',\n",
       " 'Ã©tante',\n",
       " 'Ã©tants',\n",
       " 'Ã©tantes',\n",
       " 'suis',\n",
       " 'es',\n",
       " 'est',\n",
       " 'sommes',\n",
       " 'Ãªtes',\n",
       " 'sont',\n",
       " 'serai',\n",
       " 'seras',\n",
       " 'sera',\n",
       " 'serons',\n",
       " 'serez',\n",
       " 'seront',\n",
       " 'serais',\n",
       " 'serait',\n",
       " 'serions',\n",
       " 'seriez',\n",
       " 'seraient',\n",
       " 'Ã©tais',\n",
       " 'Ã©tait',\n",
       " 'Ã©tions',\n",
       " 'Ã©tiez',\n",
       " 'Ã©taient',\n",
       " 'fus',\n",
       " 'fut',\n",
       " 'fÃ»mes',\n",
       " 'fÃ»tes',\n",
       " 'furent',\n",
       " 'sois',\n",
       " 'soit',\n",
       " 'soyons',\n",
       " 'soyez',\n",
       " 'soient',\n",
       " 'fusse',\n",
       " 'fusses',\n",
       " 'fÃ»t',\n",
       " 'fussions',\n",
       " 'fussiez',\n",
       " 'fussent',\n",
       " 'ayant',\n",
       " 'ayante',\n",
       " 'ayantes',\n",
       " 'ayants',\n",
       " 'eu',\n",
       " 'eue',\n",
       " 'eues',\n",
       " 'eus',\n",
       " 'ai',\n",
       " 'as',\n",
       " 'avons',\n",
       " 'avez',\n",
       " 'ont',\n",
       " 'aurai',\n",
       " 'auras',\n",
       " 'aura',\n",
       " 'aurons',\n",
       " 'aurez',\n",
       " 'auront',\n",
       " 'aurais',\n",
       " 'aurait',\n",
       " 'aurions',\n",
       " 'auriez',\n",
       " 'auraient',\n",
       " 'avais',\n",
       " 'avait',\n",
       " 'avions',\n",
       " 'aviez',\n",
       " 'avaient',\n",
       " 'eut',\n",
       " 'eÃ»mes',\n",
       " 'eÃ»tes',\n",
       " 'eurent',\n",
       " 'aie',\n",
       " 'aies',\n",
       " 'ait',\n",
       " 'ayons',\n",
       " 'ayez',\n",
       " 'aient',\n",
       " 'eusse',\n",
       " 'eusses',\n",
       " 'eÃ»t',\n",
       " 'eussions',\n",
       " 'eussiez',\n",
       " 'eussent']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('french') # this is for french language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bbf96f6d-7d5b-48ca-9d9c-e54665225128",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph ='''\n",
    "Technology is transforming the way we live and work,\n",
    "creating new opportunities and challenges. From artificial intelligence to blockchain,\n",
    "the rapid advancements in digital tools and platforms are reshaping industries, making processes faster \n",
    "and more efficient. This wave of innovation drives global connectivity, enabling seamless communication and collaboration across borders. \n",
    "However, it also presents ethical concerns, such as data privacy, security, and the digital divide. As we continue to embrace these changes, \n",
    "it becomes increasingly important to balance technological progress with responsible use, ensuring that innovation benefits everyone and contributes\n",
    "to a more equitable future.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "68393950-dd72-4022-9bc9-cd3863f97357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9cbb9a06-473c-4c1b-bf75-84ac58de9b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer =PorterStemmer()  #initializing the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0b18cd66-7a5a-4ff9-9a68-3a8c37107fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences= nltk.sent_tokenize(paragraph)  #converting paragraph to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "28f47ffe-db54-4470-8522-833581ae1944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nTechnology is transforming the way we live and work,\\ncreating new opportunities and challenges.',\n",
       " 'From artificial intelligence to blockchain,\\nthe rapid advancements in digital tools and platforms are reshaping industries, making processes faster \\nand more efficient.',\n",
       " 'This wave of innovation drives global connectivity, enabling seamless communication and collaboration across borders.',\n",
       " 'However, it also presents ethical concerns, such as data privacy, security, and the digital divide.',\n",
       " 'As we continue to embrace these changes, \\nit becomes increasingly important to balance technological progress with responsible use, ensuring that innovation benefits everyone and contributes\\nto a more equitable future.']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5b9f5286-7457-406a-836e-1c576565d003",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4dd4fbed-e105-4695-9563-376d9661f5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we will apply the stopwords to remove redundant words from sentences and then applying the porter stemming on the each word of sentence\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    \n",
    "    words = nltk.word_tokenize(sentences[i])   #it will store the each word from sent in words[]\n",
    "    words= [porter_stem.stem(word) for word in words if word not in set(stopwords.words(\"english\")) ]\n",
    "    sentences[i] =' '.join(words)  #converting all the list of words into sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d5452b9b-4686-4645-a4da-1fb9824681d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['technolog transform way live work , creat new opportun challeng .',\n",
       " 'from artifici intellig blockchain , rapid advanc digit tool platform reshap industri , make process faster effici .',\n",
       " 'thi wave innov drive global connect , enabl seamless commun collabor across border .',\n",
       " 'howev , also present ethic concern , data privaci , secur , digit divid .',\n",
       " 'as continu embrac chang , becom increasingli import balanc technolog progress respons use , ensur innov benefit everyon contribut equit futur .']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "87b8b18c-8e04-434b-bf96-6668bbc47c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "##let's do with the snowball stemming\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stem=SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1d3f3606-a77d-4489-b3f1-c2e3b2a4cd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we will apply the stopwords to remove redundant words from sentences and then applying the snowball stemming on the each word of sentence\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    \n",
    "    words = nltk.word_tokenize(sentences[i])   #it will store the each word from sent in words[]\n",
    "    words= [snowball_stem.stem(word) for word in words if word not in set(stopwords.words(\"english\"))]\n",
    "    sentences[i] =' '.join(words)  #converting all the list of words into sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fd53ad9c-b634-4657-a42f-f2d6509e3469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['technolog transform way live work , creat new opportun challeng .',\n",
       " 'artifici intellig blockchain , rapid advanc digit tool platform reshap industri , make process faster effici .',\n",
       " 'thi wave innov drive global connect , enabl seamless commun collabor across border .',\n",
       " 'howev , also present ethic concern , data privaci , secur , digit divid .',\n",
       " 'continu embrac chang , becom increasing import balanc technolog progress respon use , ensur innov benefit everyon contribut equit futur .']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences  #not giving proper word stems using snowball as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c23773-35a8-4263-b22d-d771030cba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a5d6a4da-7259-4cbe-a981-9fa2e6f008ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "##let's do with the lemmatization \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmitizer = WordNetLemmatizer() #creating the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0224a935-3d4e-468c-98c5-dfd4a7130243",
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we will apply the stopwords to remove redundant words from sentences and then applying the lemmatization on the each word of sentence\n",
    "\n",
    "for i in range(len(sentences)):\n",
    "    \n",
    "    words = nltk.word_tokenize(sentences[i])   #it will store the each word from sent in words[]\n",
    "    words= [lemmitizer.lemmatize(word.lower(),pos='v') for word in words if word not in set(stopwords.words(\"english\")) ]\n",
    "    sentences[i] =' '.join(words)  #converting all the list of words into sentences\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7de30dff-a023-4700-afd4-91c2efe49681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['technolog transform way live work , creat new opportun challeng .',\n",
       " 'artifici intellig blockchain , rapid advanc digit tool platform reshap industri , make process faster effici .',\n",
       " 'thi wave innov drive global connect , enabl seamless commun collabor across border .',\n",
       " 'howev , also present ethic concern , data privaci , secur , digit divid .',\n",
       " 'continu embrac chang , becom increase import balanc technolog progress respon use , ensur innov benefit everyon contribut equit futur .']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences  ##lemmatization give more accuracy then stemming in terms of getting root words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fc06a9-3a0c-4374-a373-254c64075570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5725f5a0-e456-4a44-a5e4-e381a7cf327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52850a11-123e-4d92-873e-5ccd035558c2",
   "metadata": {},
   "source": [
    "## Part Of Speech(POS) Tag"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e07b534-4e40-486b-8b81-5fb7860755e4",
   "metadata": {},
   "source": [
    "To identify which word belong to which pos tag we use nltk.pos_tag() for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a245b39d-e2dd-4c4e-92e1-cb05e79f94cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\subham.mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "260cbaa8-695b-45fa-83be-06e7ba7bf181",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences= nltk.sent_tokenize(paragraph)  #converting paragraph to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "18d68fd5-8a99-4a0f-9f99-3fc19ba99aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Technology', 'NN'), ('transforming', 'VBG'), ('way', 'NN'), ('live', 'JJ'), ('work', 'NN'), (',', ','), ('creating', 'VBG'), ('new', 'JJ'), ('opportunities', 'NNS'), ('challenges', 'NNS'), ('.', '.')]\n",
      "[('From', 'IN'), ('artificial', 'JJ'), ('intelligence', 'NN'), ('blockchain', 'NN'), (',', ','), ('rapid', 'JJ'), ('advancements', 'NNS'), ('digital', 'JJ'), ('tools', 'NNS'), ('platforms', 'NNS'), ('reshaping', 'VBG'), ('industries', 'NNS'), (',', ','), ('making', 'VBG'), ('processes', 'NNS'), ('faster', 'RBR'), ('efficient', 'NN'), ('.', '.')]\n",
      "[('This', 'DT'), ('wave', 'NN'), ('innovation', 'NN'), ('drives', 'VBZ'), ('global', 'JJ'), ('connectivity', 'NN'), (',', ','), ('enabling', 'VBG'), ('seamless', 'JJ'), ('communication', 'NN'), ('collaboration', 'NN'), ('across', 'IN'), ('borders', 'NNS'), ('.', '.')]\n",
      "[('However', 'RB'), (',', ','), ('also', 'RB'), ('presents', 'VBZ'), ('ethical', 'JJ'), ('concerns', 'NNS'), (',', ','), ('data', 'NNS'), ('privacy', 'NN'), (',', ','), ('security', 'NN'), (',', ','), ('digital', 'JJ'), ('divide', 'NN'), ('.', '.')]\n",
      "[('As', 'IN'), ('continue', 'JJ'), ('embrace', 'NN'), ('changes', 'NNS'), (',', ','), ('becomes', 'VBZ'), ('increasingly', 'RB'), ('important', 'JJ'), ('balance', 'NN'), ('technological', 'JJ'), ('progress', 'NN'), ('responsible', 'JJ'), ('use', 'NN'), (',', ','), ('ensuring', 'VBG'), ('innovation', 'NN'), ('benefits', 'NNS'), ('everyone', 'NN'), ('contributes', 'VBZ'), ('equitable', 'JJ'), ('future', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sentences)):\n",
    "    words=nltk.word_tokenize(sentences[i])\n",
    "    words=[word for word in words if word not in set(stopwords.words('english'))]\n",
    "    pos_tag=nltk.pos_tag(words)\n",
    "    print(pos_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c21ccf3b-9c31-4809-a40c-a32cae24bfa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For eg 'Technology' is NN which means Noun  , live is JJ means Adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "85c3e9f5-5cab-49c5-a542-0148f5f61812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Taj', 'NNP'), ('Mahal', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('beautiful', 'JJ'), ('Monument', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#use of pos tag if we have any sentence\n",
    "print(nltk.pos_tag(\"Taj Mahal is a beautiful Monument\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f4d37f-a70c-4e4e-8bc5-c607940396f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "271fa21a-27ae-4a36-8ed0-227bf0425fcb",
   "metadata": {},
   "source": [
    "## Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "32a5cb97-425e-4dc5-9e52-1c52ed1067dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence= \"The Effiel Tower built by Subham Mehta in 1980 was very good and located in  Paris France and the builder was from Vience and his name was Peter Smith\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4be9a355-f1dc-4a4c-afcd-068620313094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\subham.mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\subham.mehta\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "words=nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6b1921b5-e6bc-4dd5-b28f-d8dcc3730277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Effiel',\n",
       " 'Tower',\n",
       " 'built',\n",
       " 'by',\n",
       " 'Subham',\n",
       " 'Mehta',\n",
       " 'in',\n",
       " '1980',\n",
       " 'was',\n",
       " 'very',\n",
       " 'good',\n",
       " 'and',\n",
       " 'located',\n",
       " 'in',\n",
       " 'Paris',\n",
       " 'France',\n",
       " 'and',\n",
       " 'the',\n",
       " 'builder',\n",
       " 'was',\n",
       " 'from',\n",
       " 'Vience',\n",
       " 'and',\n",
       " 'his',\n",
       " 'name',\n",
       " 'was',\n",
       " 'Peter',\n",
       " 'Smith']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "82e179f1-6140-4912-9dd4-723b62a2cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_elements=nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "492f298d-1591-4b5b-a99a-187909caca0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('Effiel', 'NNP'),\n",
       " ('Tower', 'NNP'),\n",
       " ('built', 'VBN'),\n",
       " ('by', 'IN'),\n",
       " ('Subham', 'NNP'),\n",
       " ('Mehta', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('1980', 'CD'),\n",
       " ('was', 'VBD'),\n",
       " ('very', 'RB'),\n",
       " ('good', 'JJ'),\n",
       " ('and', 'CC'),\n",
       " ('located', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('Paris', 'NNP'),\n",
       " ('France', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('builder', 'NN'),\n",
       " ('was', 'VBD'),\n",
       " ('from', 'IN'),\n",
       " ('Vience', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('his', 'PRP$'),\n",
       " ('name', 'NN'),\n",
       " ('was', 'VBD'),\n",
       " ('Peter', 'NNP'),\n",
       " ('Smith', 'NNP')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f830e005-8aaa-4365-9e47-44bf2bcb17c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.ne_chunk(tag_elements).draw()   #after this i will get the NER of all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0749a197-6cff-414b-8f64-b8de5370b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b8cf04-9b95-4bd2-bae9-11ddfc2daa55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1508567b-58ba-4366-be2d-4d3e0c38c844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e424ed14-7943-41bc-9f2e-836fdb734608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d22eb3c-9a5c-4efa-9a40-d4e15d46aafd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf71c0d6-6db8-4c04-96fa-0aba4b08d2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81ec57d-ab0b-44f2-89f2-cf979f25521a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a178e2-5b4f-4384-a140-3dac97bed409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bbfa35-8d70-4788-b8a4-9dca4dbe779d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b222d1ef-cb6d-4657-a670-d733303ce3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac5e862-2fbc-47f5-be81-c1ab5dbcb827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d251ed3c-f2c5-4d58-ad3c-01fe571cab2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ba009-18c6-49a6-8cc4-5f02f2ae5d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99be02c6-b42d-408a-aa13-9d0d914d8c46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df542ff-743f-4f11-ac56-9c6bbfc484ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
